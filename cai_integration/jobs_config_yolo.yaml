# X-ray YOLO Detection Jobs Configuration
#
# This configuration defines the job sequence for YOLO-based X-ray detection on CAI:
# 1. setup_environment: Create Python venv and install dependencies (PyTorch, Ultralytics, etc.)
# 2. download_dataset: Download STCray dataset from HuggingFace
# 3. yolo_training: Convert data to YOLO format and train YOLO model
#
# Key Differences from VLM Approach:
# - No VQA generation needed (direct bbox training)
# - Much faster training (hours vs days)
# - Smaller model size (11-47MB vs 14GB)
# - Lower resource requirements (2-8GB VRAM vs 16GB+)
#
# Environment Variables:
# - FORCE_REINSTALL:  Set to "true" to force full environment reinstall (default: "false")
# - OUTPUT_NAME:      Combined dataset name under data/ (default: combined_xray_yolo)
# - MAX_PER_CLASS:    Cap STCray images per class, 0 = no cap (default: 0)
# - FORCE_REBUILD:    Rebuild combined dataset even if it exists (default: false)
# - MODEL_NAME: YOLO model variant (yolov8n, yolov8s, yolov8m, yolov11n, default: yolov8n.pt)
# - EPOCHS: Training epochs (default: 100)
# - BATCH_SIZE: Batch size (default: 16)
# - IMG_SIZE: Input image size (default: 640)
# - EXPORT_ONNX: Export to ONNX (default: false)
# - VAL_SPLIT: Validation split ratio (default: 0.2)
# - FREEZE_LAYERS: Freeze first N backbone layers (0 = disabled, 10 = YOLOv8 backbone)
# - CLS_LOSS_WEIGHT: Classification loss weight (default 0.5; raise for imbalanced data)
#
# Job Dependencies (handled automatically by CAI):
# - setup_environment (root) → download_luggage_xray + download_dataset → combine_datasets → yolo_training
# - combine_datasets is optional: skip it and set parent_job_key on yolo_training to download_luggage_xray
# - When parent job succeeds, child jobs auto-trigger automatically

jobs:
  # Job 1: Setup Python Environment (ROOT JOB)
  setup_environment:
    name: "Setup Python Environment"
    description: "Create Python venv and install dependencies (PyTorch, Ultralytics YOLO, etc.)"
    script: "cai_integration/setup_environment.py"
    kernel: "python3"
    cpu: 4
    memory: 16
    timeout: 3600  # 1 hour for pip installs
    parent_job_key: null
    runtime_identifier: "docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-cuda:2026.01.1-b6"
    environment:
      FORCE_REINSTALL: "false"

  # Job 2a: Download Luggage X-ray Dataset (Optional - for luggage_xray dataset)
  download_luggage_xray:
    name: "Download Luggage X-ray Dataset"
    description: "Download Luggage X-ray dataset from Roboflow (7k images with 12 classes including 5 threats)"
    script: "cai_integration/download_luggage_xray.py"
    kernel: "python3"
    cpu: 4
    memory: 8
    timeout: 3600  # 1 hour for download
    parent_job_key: "setup_environment"
    runtime_identifier: "docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-cuda:2026.01.1-b6"

  # Job 2b: Prepare STCray Dataset (RAR files pulled from Git LFS by setup_environment)
  download_dataset:
    name: "Download STCray Dataset"
    description: "Download STCray dataset from HuggingFace (46k images with 21 threat categories)"
    script: "cai_integration/download_dataset.py"
    kernel: "python3"
    cpu: 4
    memory: 8
    timeout: 3600  # 1 hour for download
    parent_job_key: "setup_environment"
    runtime_identifier: "docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-cuda:2026.01.1-b6"

  # Job 2c: Combine Datasets
  # Merges luggage_xray_yolo + STCray into a unified 16-class dataset (~36K+ images).
  # Always runs before yolo_training; skips instantly when dataset already exists (FORCE_REBUILD=false).
  # Prerequisites: download_luggage_xray AND download_dataset must both complete first.
  combine_datasets:
    name: "Combine X-ray Datasets"
    description: "Merge luggage_xray (6K) + STCray threats (30K) into unified 16-class YOLO dataset (~32K+ images)"
    script: "cai_integration/combine_datasets.py"
    kernel: "python3"
    cpu: 8   # High CPU for parallel image copying
    memory: 32  # Large dataset requires sufficient memory
    timeout: 7200  # 2 hours for 36K+ image copy + conversion
    parent_job_key: "download_luggage_xray"  # Requires luggage dataset; STCray assumed already present
    runtime_identifier: "docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-cuda:2026.01.1-b6"
    environment:
      OUTPUT_NAME: "combined_xray_yolo"  # Output dir under data/
      MAX_PER_CLASS: "2000"  # Cap images per class to balance the dataset; 0 = no cap
      FORCE_REBUILD: "true"  # Rebuild since MAX_PER_CLASS changed; set "false" after first run
      INCLUDE_LUGGAGE: "true"
      INCLUDE_STCRAY: "true"

  # ──────────────────────────────────────────────────────────────────────────
  # Job 3: YOLO Training
  #
  # Switch between training paths using ONLY the DATASET env var — no YAML
  # edits needed after the jobs are created.  combine_datasets is always the
  # parent; it completes in seconds when the combined dataset already exists
  # (FORCE_REBUILD=false), so it adds negligible overhead on PATH A runs.
  #
  #  PATH A  DATASET=luggage_xray        6,164 imgs · 12 classes · fastest
  #  PATH B  DATASET=combined_xray_yolo  36K+ imgs  · 16 classes · accurate
  #
  # Change DATASET in the CAI job's environment variables page (no redeploy).
  #
  # Model selection: yolov8n (fastest), yolov8s (balanced), yolov8m (accurate)
  # Early stopping: terminates after PATIENCE epochs without mAP50 improvement
  # Shared memory:  10 GB recommended in CAI project settings (8 workers)
  # ──────────────────────────────────────────────────────────────────────────
  yolo_training:
    name: "Train YOLO Detection Model"
    description: "Train YOLO object detection model with early stopping and backbone freezing"
    script: "cai_integration/yolo_training.py"
    kernel: "python3"
    cpu: 8
    memory: 32
    gpu: 1
    timeout: 108000  # 30 hours — combined dataset at 200 epochs takes ~27 hrs
    # Fixed parent — combine_datasets is a no-op when dataset already exists,
    # so this dependency costs nothing on PATH A (luggage_xray) runs.
    parent_job_key: "combine_datasets"
    runtime_identifier: "docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-cuda:2026.01.1-b6"
    environment:
      # ── Switch training path via this single env var ─────────────────────
      # PATH A (YOLO-only):  luggage_xray
      # PATH B (Combined):   combined_xray_yolo
      DATASET: "combined_xray_yolo"

      MODEL_NAME: "yolov8m.pt"  # yolov8n, yolov8s, yolov8m, yolov8x, yolov11n
      EPOCHS: "200"             # Max epochs (early stop if no improvement)
      BATCH_SIZE: "16"          # 16 for yolov8m, 8 for yolov8x, 32 for yolov8n/s
      IMG_SIZE: "640"
      EXPORT_ONNX: "false"      # Set "true" to export ONNX for production
      VAL_SPLIT: "0.2"          # Only used for stcray dataset

      # Hyperparameters (optimised for combined dataset run 2)
      LEARNING_RATE: "0.001"    # Lowered from 0.003; more stable with frozen backbone
      OPTIMIZER: "AdamW"        # SGD | Adam | AdamW
      PATIENCE: "20"            # Increased from 10; gives plateau more room to resolve
      WARMUP_EPOCHS: "5.0"

      # Backbone freezing — keeps ImageNet features intact while head learns X-ray patterns
      # 10 = freeze YOLOv8 backbone (layers 0-9); 0 = disabled
      FREEZE_LAYERS: "10"

      # Class loss weight — raised to improve class discrimination on balanced dataset
      CLS_LOSS_WEIGHT: "1.5"    # Default 0.5; increase when cls_loss is dominant

      # Augmentation
      AUG_DEGREES: "10.0"       # Rotation (0–30°)
      AUG_TRANSLATE: "0.05"     # Translation (0–0.2)
      AUG_SCALE: "0.3"          # Scale (0–1.0)
      AUG_MOSAIC: "0.8"         # Mosaic probability (0–1.0)
      AUG_MIXUP: "0.0"          # Mixup probability (0–1.0)

  # Job 4: Deploy YOLO API as CAI Application
  # Creates a persistent API endpoint accessible at https://[subdomain].[cai-domain]
  deploy_yolo_application:
    name: "Deploy YOLO API Application"
    description: "Deploy trained YOLO model as persistent CAI Application with exposed endpoints"
    script: "cai_integration/deploy_yolo_application.py"
    kernel: "python3"
    cpu: 2
    memory: 8
    timeout: 1800  # 30 minutes
    parent_job_key: "yolo_training"
    runtime_identifier: "docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-cuda:2026.01.1-b6"
    environment:
      # CAI credentials (set in GitHub Secrets or CAI Environment Variables)
      CAI_API_KEY: "${CAI_API_KEY}"
      CAI_DOMAIN: "${CAI_DOMAIN}"
      CAI_PROJECT_NAME: "xray-scanning-model"  # Deploy to this project (matches setup_project.py)
      # Application configuration
      MODEL_PATH: ""  # Auto-detect latest trained model, or use pre-trained (e.g., "yolov8n.pt")
      APP_SUBDOMAIN: "xray-yolo-api"  # Custom subdomain (optional)
      USE_GPU: "true"  # Set to "false" for CPU-only deployment (slower but no GPU needed)

# Note: After deployment, the API will be available at:
# - Health Check:     https://xray-yolo-api.[cai-domain]/health
# - API Docs:         https://xray-yolo-api.[cai-domain]/docs
# - OpenAI API:       https://xray-yolo-api.[cai-domain]/v1/chat/completions
# - Direct Detection: https://xray-yolo-api.[cai-domain]/v1/detect
