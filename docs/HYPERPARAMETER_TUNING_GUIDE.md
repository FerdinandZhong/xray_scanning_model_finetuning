# Hyperparameter Tuning Guide for YOLO Training

**Easy Configuration:** All key hyperparameters are now configurable via environment variables in `cai_integration/jobs_config_yolo.yaml`

---

## Quick Start

### Edit Hyperparameters

```yaml
# File: cai_integration/jobs_config_yolo.yaml

environment:
  # Just change these values - no need to edit Python code!
  LEARNING_RATE: "0.002"    # Try: 0.001, 0.002, 0.005
  OPTIMIZER: "AdamW"        # Try: SGD, Adam, AdamW
  PATIENCE: "10"            # Try: 5, 10, 15, 20
  AUG_DEGREES: "10.0"       # Try: 5.0, 10.0, 15.0
  AUG_MOSAIC: "0.8"         # Try: 0.5, 0.8, 1.0
```

### Trigger Training

```bash
# Commit changes
git add cai_integration/jobs_config_yolo.yaml
git commit -m "Adjust training hyperparameters"
git push

# Trigger via GitHub Actions
# Workflow: Deploy X-ray Detection to CAI
```

---

## Configurable Parameters

### 1. Optimizer Settings

#### LEARNING_RATE (lr0)

**What it does:** Controls how much to adjust model weights during training

```yaml
LEARNING_RATE: "0.002"  # Default (optimized)
```

| Value | When to Use | Expected Behavior |
|-------|-------------|-------------------|
| **0.001** | Training too unstable | Very slow but stable convergence |
| **0.002** | ‚úÖ **Recommended for yolov8x** | Balanced speed and stability |
| **0.005** | Model not learning fast enough | Faster but may overshoot |
| **0.01** | Small models (yolov8n) | Fast learning, risky for large models |

**Symptoms:**
- ‚ùå **Too high (>0.005):** Loss oscillates, mAP50 unstable, can't converge
- ‚ùå **Too low (<0.001):** Training very slow, mAP50 improves too slowly
- ‚úÖ **Just right:** Steady mAP50 increase, loss decreasing smoothly

#### OPTIMIZER

**What it does:** Algorithm for updating model weights

```yaml
OPTIMIZER: "AdamW"  # Default (optimized)
```

| Value | Pros | Cons | When to Use |
|-------|------|------|-------------|
| **AdamW** | ‚úÖ Adaptive LR per parameter<br>‚úÖ Great for large models<br>‚úÖ Handles sparse gradients | More memory | ‚úÖ **Recommended for yolov8x** |
| **Adam** | Adaptive LR<br>Faster than SGD | Less regularization | Medium models (yolov8m) |
| **SGD** | Simple, reliable<br>Less memory | Requires careful LR tuning | Small models (yolov8n) |
| **auto** | YOLO picks automatically | May not be optimal | Quick experiments |

**Recommendation:**
- YOLOv8x: **AdamW** (best performance)
- YOLOv8m/s: Adam or AdamW
- YOLOv8n: SGD or auto

#### PATIENCE (Early Stopping)

**What it does:** Stops training after N epochs without mAP50 improvement

```yaml
PATIENCE: "10"  # Default (optimized)
```

| Value | When to Use | Expected Stop Time |
|-------|-------------|-------------------|
| **5** | Quick experiments, testing hyperparameters | Epoch 30-60 |
| **10** | ‚úÖ **Recommended (balanced)** | Epoch 80-140 |
| **15** | Conservative, allow fluctuations | Epoch 100-180 |
| **20** | Very patient, ensure full convergence | Epoch 120-200 |

**Trade-off:**
- Lower patience: Faster feedback, may stop too early
- Higher patience: More GPU time, ensures convergence

#### WARMUP_EPOCHS

**What it does:** Gradually increases learning rate from 0 to LEARNING_RATE over N epochs

```yaml
WARMUP_EPOCHS: "5.0"  # Default (optimized)
```

| Value | When to Use | Effect |
|-------|-------------|--------|
| **3.0** | Small models, simple datasets | Quick start |
| **5.0** | ‚úÖ **Recommended for yolov8x** | Smooth gradual warmup |
| **10.0** | Very large models, unstable training | Very gentle start |

**Why warmup matters:**
- Prevents early training instability
- Allows optimizer to find good initial direction
- Critical for large models like yolov8x

---

### 2. Augmentation Parameters

#### AUG_DEGREES (Rotation)

**What it does:** Random rotation of images during training

```yaml
AUG_DEGREES: "10.0"  # Default (optimized)
```

| Value | Effect | When to Use |
|-------|--------|-------------|
| **0.0** | No rotation | X-rays have fixed orientation |
| **5.0** | Slight rotation | Minimal variation expected |
| **10.0** | ‚úÖ **Recommended (moderate)** | Baggage can be tilted |
| **15.0** | Aggressive rotation | Highly variable orientations |
| **30.0** | Extreme rotation | May confuse model |

**Trade-off:**
- Higher: Better generalization, slower convergence
- Lower: Faster convergence, may overfit to orientation

#### AUG_TRANSLATE (Translation)

**What it does:** Random horizontal/vertical shift of images

```yaml
AUG_TRANSLATE: "0.05"  # Default (optimized)
```

| Value | Effect | When to Use |
|-------|--------|-------------|
| **0.0** | No translation | Objects always centered |
| **0.05** | ‚úÖ **Recommended (slight shift)** | Objects mostly centered |
| **0.1** | Moderate shift | Objects can be off-center |
| **0.2** | Large shift | Very variable positioning |

**Note:** 0.05 = 5% of image size (32 pixels for 640x640)

#### AUG_SCALE (Scaling)

**What it does:** Random zoom in/out

```yaml
AUG_SCALE: "0.3"  # Default (optimized)
```

| Value | Effect | When to Use |
|-------|--------|-------------|
| **0.0** | No scaling | Objects same size |
| **0.3** | ‚úÖ **Recommended (moderate)** | Objects vary in size |
| **0.5** | Aggressive scaling | Wide size variation |
| **1.0** | Extreme scaling | May lose small objects |

**Note:** 0.3 = ¬±30% size variation

#### AUG_MOSAIC (Mosaic Augmentation)

**What it does:** Combines 4 images into one for training

```yaml
AUG_MOSAIC: "0.8"  # Default (optimized)
```

| Value | Effect | When to Use |
|-------|--------|-------------|
| **0.0** | Disabled | Single object per image |
| **0.5** | 50% of batches | Moderate multi-object learning |
| **0.8** | ‚úÖ **Recommended (aggressive)** | Diverse scenes expected |
| **1.0** | Always enabled | Maximum augmentation |

**Benefits:**
- Improves multi-object detection
- Better context learning
- Regularization effect

**Drawback:**
- Can confuse model on small objects
- Slower convergence

#### AUG_MIXUP (Mixup Augmentation)

**What it does:** Blends two images together

```yaml
AUG_MIXUP: "0.0"  # Default (optimized) - DISABLED
```

| Value | Effect | When to Use |
|-------|--------|-------------|
| **0.0** | ‚úÖ **Recommended (disabled)** | Small/fine-grained objects (threats) |
| **0.1** | 10% of batches | Large objects, simple classes |
| **0.2** | 20% of batches | Very simple detection tasks |

**Why disabled for X-ray:**
- Threats are small and detailed
- Blending confuses threat boundaries
- X-ray images already low contrast

---

## Preset Configurations

### Preset 1: Conservative (Stability First) üê¢

**When to use:** First attempt, uncertain about convergence

```yaml
LEARNING_RATE: "0.001"    # Lower LR
OPTIMIZER: "AdamW"
PATIENCE: "20"            # Very patient
WARMUP_EPOCHS: "10.0"     # Long warmup
AUG_DEGREES: "5.0"        # Minimal augmentation
AUG_TRANSLATE: "0.03"
AUG_SCALE: "0.2"
AUG_MOSAIC: "0.5"
AUG_MIXUP: "0.0"
```

**Expected:**
- Very stable training
- Slow but steady convergence
- May take 150-200 epochs
- Final mAP50: 0.45-0.52

---

### Preset 2: Balanced (Recommended) ‚ö°

**When to use:** Default for most cases

```yaml
LEARNING_RATE: "0.002"    # Balanced
OPTIMIZER: "AdamW"
PATIENCE: "10"            # Moderate
WARMUP_EPOCHS: "5.0"      # Standard warmup
AUG_DEGREES: "10.0"       # Moderate augmentation
AUG_TRANSLATE: "0.05"
AUG_SCALE: "0.3"
AUG_MOSAIC: "0.8"
AUG_MIXUP: "0.0"
```

**Expected:**
- Balanced speed and stability
- Converges in 100-150 epochs
- Final mAP50: 0.50-0.55
- ‚úÖ **This is the current default**

---

### Preset 3: Aggressive (Speed First) üöÄ

**When to use:** Need quick results, willing to risk instability

```yaml
LEARNING_RATE: "0.005"    # Higher LR
OPTIMIZER: "Adam"         # Faster than AdamW
PATIENCE: "5"             # Quick exit
WARMUP_EPOCHS: "3.0"      # Short warmup
AUG_DEGREES: "15.0"       # Heavy augmentation
AUG_TRANSLATE: "0.1"
AUG_SCALE: "0.5"
AUG_MOSAIC: "1.0"
AUG_MIXUP: "0.1"
```

**Expected:**
- Fast convergence or failure
- May stop early (epoch 40-80)
- Final mAP50: 0.40-0.50 (if lucky)
- ‚ö†Ô∏è Higher risk of poor convergence

---

### Preset 4: Small Model (YOLOv8n) üèÉ

**When to use:** Training yolov8n specifically

```yaml
MODEL_NAME: "yolov8n.pt"
LEARNING_RATE: "0.01"     # Higher LR for small model
OPTIMIZER: "SGD"          # SGD works well for small models
BATCH_SIZE: "16"          # Can use larger batch
PATIENCE: "15"
WARMUP_EPOCHS: "3.0"
AUG_DEGREES: "15.0"       # Small models benefit from more aug
AUG_TRANSLATE: "0.1"
AUG_SCALE: "0.5"
AUG_MOSAIC: "1.0"
AUG_MIXUP: "0.0"
```

---

## Hyperparameter Tuning Strategy

### Step 1: Start with Default (Balanced Preset)

```yaml
# Use current default values
# Run training, monitor mAP50
```

**If mAP50 > 0.50:** ‚úÖ Success! Use the model  
**If mAP50 = 0.40-0.50:** üü° Borderline, try tweaking  
**If mAP50 < 0.40:** ‚ùå Need to adjust

### Step 2: Diagnose Issues

| Symptom | Diagnosis | Fix |
|---------|-----------|-----|
| **Loss oscillating wildly** | LR too high | Reduce LEARNING_RATE by 50% |
| **mAP50 increasing very slowly** | LR too low | Increase LEARNING_RATE by 50% |
| **Training stops too early (<50 epochs)** | Patience too low | Increase PATIENCE to 15-20 |
| **mAP50 plateaus early (<0.30)** | Bad convergence | Change OPTIMIZER or reduce LR |
| **Train mAP >> Val mAP** | Overfitting | Increase augmentation or reduce model size |
| **Both train & val mAP low** | Underfitting | Reduce augmentation or increase LR |

### Step 3: Iterative Tuning

**Example tuning session:**

```yaml
# Attempt 1: Default
LEARNING_RATE: "0.002"
Result: mAP50 = 0.35 (low)
Diagnosis: LR might be slightly too low

# Attempt 2: Increase LR
LEARNING_RATE: "0.003"
Result: mAP50 = 0.48 (better!)
Diagnosis: Almost there

# Attempt 3: Reduce augmentation
LEARNING_RATE: "0.003"
AUG_DEGREES: "8.0"
AUG_TRANSLATE: "0.03"
Result: mAP50 = 0.52 ‚úÖ Success!
```

---

## Parameter Interaction Guide

### Learning Rate + Optimizer

| Optimizer | Recommended LR Range | Why |
|-----------|---------------------|-----|
| **AdamW** | 0.001 - 0.003 | Adaptive, handles high LR better |
| **Adam** | 0.001 - 0.003 | Similar to AdamW |
| **SGD** | 0.005 - 0.01 | Needs higher LR, less adaptive |

### Model Size + Learning Rate

| Model | Parameters | Recommended LR | Batch Size |
|-------|------------|----------------|------------|
| YOLOv8n | 3.2M | 0.01 | 16-32 |
| YOLOv8s | 11.2M | 0.005 | 16-24 |
| YOLOv8m | 25.9M | 0.003 | 8-16 |
| **YOLOv8x** | **68.2M** | **0.002** | **4-8** |

### Dataset Size + Augmentation

| Training Images | Aug Level | AUG_DEGREES | AUG_MOSAIC |
|----------------|-----------|-------------|------------|
| < 1,000 | Heavy | 15.0 | 1.0 |
| 1,000 - 5,000 | Moderate | 10.0 | 0.8 |
| **6,164 (yours)** | **Moderate** | **10.0** | **0.8** |
| > 10,000 | Light | 5.0 | 0.5 |

---

## Common Scenarios

### Scenario 1: Training Stops Too Early (Epoch <60)

**Symptom:**
```
Epoch 45: mAP50 = 0.25, patience = 0/10
Epoch 55: mAP50 = 0.25, patience = 10/10 ‚Üí STOP
Final: mAP50 = 0.25 (too low!)
```

**Diagnosis:** Converged too early to suboptimal solution

**Fix Option A:** Increase patience
```yaml
PATIENCE: "20"  # Give more time before stopping
```

**Fix Option B:** Adjust learning rate
```yaml
LEARNING_RATE: "0.003"  # Slightly higher for faster convergence
PATIENCE: "15"
```

---

### Scenario 2: Loss Oscillating

**Symptom:**
```
Epoch 20: Loss = 2.5
Epoch 21: Loss = 3.1  ‚Üê Increased!
Epoch 22: Loss = 2.3
Epoch 23: Loss = 3.5  ‚Üê Oscillating
```

**Diagnosis:** Learning rate too high

**Fix:**
```yaml
LEARNING_RATE: "0.001"  # Reduce by 50%
# Or try smaller batch size:
BATCH_SIZE: "4"
```

---

### Scenario 3: Very Slow Learning

**Symptom:**
```
Epoch 50: mAP50 = 0.12
Epoch 100: mAP50 = 0.18
Epoch 150: mAP50 = 0.22 (barely improving)
```

**Diagnosis:** Learning rate too low or too much augmentation

**Fix Option A:** Increase learning rate
```yaml
LEARNING_RATE: "0.003"  # Increase by 50%
```

**Fix Option B:** Reduce augmentation
```yaml
AUG_DEGREES: "5.0"
AUG_TRANSLATE: "0.03"
AUG_SCALE: "0.2"
AUG_MOSAIC: "0.5"
```

---

### Scenario 4: Overfitting

**Symptom:**
```
Train mAP50: 0.65 (high!)
Val mAP50: 0.35 (low!)
Gap: 0.30 (large gap indicates overfitting)
```

**Diagnosis:** Model memorizing training data

**Fix Option A:** Increase augmentation
```yaml
AUG_DEGREES: "15.0"
AUG_TRANSLATE: "0.1"
AUG_SCALE: "0.5"
AUG_MOSAIC: "1.0"
AUG_MIXUP: "0.1"  # Re-enable mixup
```

**Fix Option B:** Use smaller model
```yaml
MODEL_NAME: "yolov8m.pt"  # Switch to medium
```

---

## Model-Specific Recommendations

### For YOLOv8x (Your Current Model)

```yaml
# Optimized configuration for yolov8x on 6,164 images
MODEL_NAME: "yolov8x.pt"
EPOCHS: "200"
BATCH_SIZE: "8"

# These are already optimized for yolov8x:
LEARNING_RATE: "0.002"    # ‚úÖ Optimal
OPTIMIZER: "AdamW"        # ‚úÖ Best for large models
PATIENCE: "10"            # ‚úÖ Balanced
WARMUP_EPOCHS: "5.0"      # ‚úÖ Good for 68M params
AUG_DEGREES: "10.0"       # ‚úÖ Moderate
AUG_TRANSLATE: "0.05"     # ‚úÖ Moderate
AUG_SCALE: "0.3"          # ‚úÖ Moderate
AUG_MOSAIC: "0.8"         # ‚úÖ Good for complex scenes
AUG_MIXUP: "0.0"          # ‚úÖ Disabled (threats are small)
```

**Expected: mAP50 0.50-0.55, accuracy 60-75%**

### For YOLOv8m (Alternative)

```yaml
MODEL_NAME: "yolov8m.pt"
EPOCHS: "150"
BATCH_SIZE: "16"  # Can use larger batch

LEARNING_RATE: "0.003"    # Slightly higher
OPTIMIZER: "AdamW"
PATIENCE: "15"
WARMUP_EPOCHS: "5.0"
AUG_DEGREES: "12.0"       # Slightly more
AUG_TRANSLATE: "0.08"
AUG_SCALE: "0.4"
AUG_MOSAIC: "0.9"
AUG_MIXUP: "0.0"
```

**Expected: mAP50 0.45-0.55, accuracy 60-70%**

### For YOLOv8n (Small/Fast)

```yaml
MODEL_NAME: "yolov8n.pt"
EPOCHS: "150"
BATCH_SIZE: "32"  # Can use much larger batch

LEARNING_RATE: "0.01"     # Much higher
OPTIMIZER: "SGD"          # SGD works well
PATIENCE: "20"
WARMUP_EPOCHS: "3.0"
AUG_DEGREES: "15.0"       # More augmentation helps
AUG_TRANSLATE: "0.1"
AUG_SCALE: "0.5"
AUG_MOSAIC: "1.0"
AUG_MIXUP: "0.0"
```

**Expected: mAP50 0.35-0.45, accuracy 40-55%**

---

## Advanced Tuning

### Learning Rate Schedule

**Current (Fixed):**
```python
lr0 = 0.002      # Initial
lrf = 0.001      # Final (lr0 * lrf = 0.000002)
```

**How it works:**
- LR decreases linearly from lr0 to (lr0 * lrf) over epochs
- Epoch 1: LR = 0.002
- Epoch 100: LR = 0.000002
- Epoch 200: LR = 0.000002

**If you want to customize lrf:**
- Edit `training/train_yolo.py` line 117:
  ```python
  lrf=0.01,  # Final LR multiplier (try 0.001, 0.01, 0.1)
  ```

### Batch Size vs Learning Rate

**Rule of thumb:** When changing batch size, adjust LR proportionally

```yaml
# If batch=16, lr=0.002
BATCH_SIZE: "16"
LEARNING_RATE: "0.002"

# If batch=8, reduce lr
BATCH_SIZE: "8"
LEARNING_RATE: "0.001"  # Half the batch ‚Üí half the LR

# If batch=32, increase lr
BATCH_SIZE: "32"
LEARNING_RATE: "0.004"  # Double the batch ‚Üí double the LR
```

**Current config (optimized for yolov8x):**
```yaml
BATCH_SIZE: "8"
LEARNING_RATE: "0.002"  # Well-matched
```

---

## Testing Different Configurations

### Quick Experimentation

```bash
# Test 1: Default config
# Edit jobs_config_yolo.yaml with Balanced preset
git add cai_integration/jobs_config_yolo.yaml
git commit -m "Test: balanced hyperparameters"
git push
# Trigger training, wait 6 hours, check mAP50 @ epoch 100

# Test 2: If mAP50 < 0.40, try conservative
# Edit jobs_config_yolo.yaml with Conservative preset
git add cai_integration/jobs_config_yolo.yaml
git commit -m "Test: conservative hyperparameters"
git push
# Trigger training

# Test 3: If mAP50 still low, investigate data quality
```

### A/B Testing

**Run two configurations in parallel:**

```yaml
# Configuration A (default): yolov8x + AdamW + lr=0.002
# Configuration B (alternative): yolov8m + AdamW + lr=0.003

# Compare results after both complete
```

---

## Troubleshooting

### Issue: OOM (Out of Memory) Error

**Error:** CUDA out of memory

**Fix:**
```yaml
BATCH_SIZE: "4"  # Reduce batch size
# Or reduce workers in train_yolo.py: workers=4
```

### Issue: Training Very Slow

**Symptom:** 10+ seconds per epoch

**Fix:**
```yaml
# Check:
# 1. GPU being used? (Check CAI job logs)
# 2. Workers causing slowdown?
# 3. Data loading bottleneck?

# Try:
BATCH_SIZE: "16"  # Increase batch size
# Edit train_yolo.py: workers=4 (reduce workers)
```

### Issue: mAP50 Stuck at Same Value

**Symptom:** mAP50 = 0.20 for 30+ epochs

**Fix:**
```yaml
# Try completely different config:
LEARNING_RATE: "0.001"  # Much lower
OPTIMIZER: "SGD"        # Different optimizer
PATIENCE: "20"          # More patient
```

---

## Configuration Template

### Copy-Paste Ready Configuration

```yaml
# cai_integration/jobs_config_yolo.yaml
# Edit these values as needed:

environment:
  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  # BASIC CONFIGURATION
  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  MODEL_NAME: "yolov8x.pt"  # Model size
  DATASET: "luggage_xray"   # Dataset name
  EPOCHS: "200"             # Max epochs
  BATCH_SIZE: "8"           # Batch size
  IMG_SIZE: "640"           # Image size
  
  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  # HYPERPARAMETERS (Tune these!)
  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  LEARNING_RATE: "0.002"    # Try: 0.001-0.005
  OPTIMIZER: "AdamW"        # Try: SGD, Adam, AdamW
  PATIENCE: "10"            # Try: 5-20
  WARMUP_EPOCHS: "5.0"      # Try: 3.0-10.0
  
  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  # AUGMENTATION (Fine-tune if needed)
  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  AUG_DEGREES: "10.0"       # Rotation: 0-30
  AUG_TRANSLATE: "0.05"     # Translation: 0-0.2
  AUG_SCALE: "0.3"          # Scale: 0-1.0
  AUG_MOSAIC: "0.8"         # Mosaic: 0-1.0
  AUG_MIXUP: "0.0"          # Mixup: 0-0.3 (keep 0 for X-ray)
```

---

## Monitoring & Validation

### What to Watch During Training

```
‚úì Good signs:
  - mAP50 increasing steadily
  - Loss decreasing smoothly
  - Patience counter resetting often (finding improvements)
  - Train/Val loss similar (no overfitting)

‚ö†Ô∏è Warning signs:
  - mAP50 oscillating wildly ‚Üí Reduce LR
  - Loss not decreasing ‚Üí Check optimizer/LR
  - Early stop at <60 epochs ‚Üí Increase patience or adjust LR

‚ùå Red flags:
  - Loss = NaN ‚Üí LR too high, restart
  - mAP50 stuck at 0.20 ‚Üí Change optimizer/LR
  - Train mAP >> Val mAP ‚Üí Overfitting, increase aug
```

### After Training

```bash
# Test the model
python scripts/test_yolo_with_threshold.py \
    --conf-threshold 0.25

# Check results:
# - Overall Accuracy: >60% is good
# - mAP50 (from training logs): >0.50 is production-ready
# - Threats Found: >35 (out of 50) is good
```

---

## Quick Reference

### Default Values (Current Optimized Config)

```yaml
LEARNING_RATE: "0.002"    # Optimized for yolov8x
OPTIMIZER: "AdamW"        # Best for large models
PATIENCE: "10"            # Balanced (saves GPU time)
WARMUP_EPOCHS: "5.0"      # Good warmup period
AUG_DEGREES: "10.0"       # Moderate rotation
AUG_TRANSLATE: "0.05"     # Slight translation
AUG_SCALE: "0.3"          # Moderate scaling
AUG_MOSAIC: "0.8"         # Aggressive mosaic
AUG_MIXUP: "0.0"          # Disabled for threats
```

### One-Line Adjustments

```yaml
# Want faster training?
PATIENCE: "5"

# Want more stable training?
LEARNING_RATE: "0.001"
PATIENCE: "20"

# Want faster convergence?
LEARNING_RATE: "0.003"
WARMUP_EPOCHS: "3.0"

# Want better generalization?
AUG_DEGREES: "15.0"
AUG_TRANSLATE: "0.1"
AUG_SCALE: "0.5"
```

---

## Summary

### What's Now Configurable

‚úÖ **9 key hyperparameters** configurable via environment variables  
‚úÖ **No Python code editing needed** - just edit YAML  
‚úÖ **Instant changes** - commit and trigger new training  
‚úÖ **Optimized defaults** already set for your use case

### Files Updated

```
‚úÖ training/train_yolo.py
   - Added argparse arguments for hyperparameters
   - Updated function signature
   - Parameters now configurable via CLI

‚úÖ cai_integration/yolo_training.py
   - Reads hyperparameters from environment variables
   - Passes to train_yolo.py via command line
   - Enhanced logging

‚úÖ cai_integration/jobs_config_yolo.yaml
   - All hyperparameters exposed as environment variables
   - Documented with comments and recommended ranges
```

### Example: Tune Your Config

```bash
# 1. Edit hyperparameters
vim cai_integration/jobs_config_yolo.yaml
# Change LEARNING_RATE, OPTIMIZER, etc.

# 2. Commit
git add cai_integration/jobs_config_yolo.yaml
git commit -m "Tune: increase learning rate to 0.003"

# 3. Push and trigger
git push
# Run GitHub Actions workflow

# 4. Monitor results
# Check mAP50 after 6 hours
```

---

**Ready to tune!** Just edit `jobs_config_yolo.yaml` and trigger training. üöÄ
